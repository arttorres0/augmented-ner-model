{"cells":[{"cell_type":"code","source":["!pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ox2M6Jb9H5iZ","executionInfo":{"status":"ok","timestamp":1658178231476,"user_tz":240,"elapsed":3826,"user":{"displayName":"Altigran Soares","userId":"17972430181076780193"}},"outputId":"abd2195a-8206-4ca7-95b1-b6cd2c5a274d"},"id":"ox2M6Jb9H5iZ","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"]}]},{"cell_type":"code","source":["from os import listdir\n","# files=[ '101016jmatlet201210091',\n","#         '101016jsolidstatesciences201609005',\n","#         '101016jmolcata201411015',\n","#         '101016jjallcom201605264',\n","#         '101016jjpowsour201512048',\n","#         '101016jjpowsour201411037',\n","#         '101002aenm201300174',\n","#         '101039c4cy00360h',\n","#         '101039c5tc00196j',\n","#         '101038srep02426',\n","#         '101016jjssc200911025',\n","#         '101016jssc200704044',\n","#         '101016jnanoen201410008',\n","#         '101016jmatlet201701142',\n","#         '101039c5ta08878j'\n","# ]\n","files=[ \n","        '101016jssc200704044'\n","]\n","\n","ann_files= [f for f in listdir('data') if f.endswith(\".ann\") and f[:-4] in files]\n","\n","# print(ann_files)\n","print(len(ann_files))\n","\n","count = 2127\n","result = []\n","\n","testeeee = 0\n","testeeee2 = 0\n","\n","# ann_files = ['101016jjpowsour201203049.ann']\n","\n","for ann_file in ann_files:\n","    txt_file = f\"{ann_file.split('.ann')[0]}.txt\"\n","    print(f\"Processing files: {ann_file}, {txt_file}\")\n","\n","    f1 = open(f\"data/{ann_file}\", \"r\")\n","    f2 = open(f\"data/{txt_file}\", \"r\")\n","\n","    tags = []\n","\n","    f1_lines = f1.readlines()\n","    for line in f1_lines:\n","        columns = line.split(\"\\t\")\n","        if(len(columns) > 3):\n","            raise \"ERROR1\"\n","        if(columns[0].startswith(\"T\")):\n","            # print(columns)\n","\n","            tag = columns[1].split()\n","            # print(tag)\n","            if(len(tag) > 3):\n","                raise \"ERROR2\"\n","\n","            # tags.append(tag[0])\n","            # tags_indexes.append((int(tag[1]), int(tag[2])))\n","    \n","            # words.append([word for word in columns[2].replace(\"\\n\", \"\").split()])\n","\n","            tags.append({\n","                'tag': f\"{tag[0]}\",\n","                'tags_indexes': (int(tag[1]), int(tag[2])),\n","                'word': [word for word in columns[2].replace(\"\\n\", \"\").split()]\n","            })\n","            # print(words)\n","\n","    print(tags)\n","\n","    import nltk\n","    nltk.download('punkt')\n","    from nltk.tokenize import TreebankWordTokenizer as twt\n","\n","    f2_lines = f2.readlines()\n","    # print(nltk.sent_tokenize(f2_lines[1]))\n","    testeeee = testeeee + len(f2_lines)\n","    teste_sentences = [nltk.sent_tokenize(line) for line in f2_lines]\n","    # print(teste_sentences)\n","    testeeee2 = testeeee2 + sum([len(line) for line in teste_sentences])\n","    # print(teste_sentences)\n","    teste_sentences = [line if len(line)>0 else ' ' for line in teste_sentences]\n","    # print(teste_sentences)\n","    teste_sentences = [i for j in teste_sentences for i in j]\n","    # print(teste_sentences)\n","    # print([list(twt().span_tokenize(line)) for line in f2_lines])\n","    # print(len(f2_lines))\n","    line_lengths = [list(twt().span_tokenize(line))[-1][1] if len(list(twt().span_tokenize(line))) else 0 for line in teste_sentences]\n","    # print(line_lengths)\n","    line_end_indexes = [ i+sum(line_lengths[0:k])+k if k!=0 else i for k,i in enumerate(line_lengths)]\n","    # print(line_end_indexes)\n","    erase_lines = [k for k, line in enumerate(line_lengths) if line==0]\n","    # print(erase_lines)\n","    line_end_indexes = [line for k, line in enumerate(line_end_indexes) if k not in erase_lines]\n","    # print(line_end_indexes)\n","    # print(line_end_indexes)\n","    teste=\" \".join(line.strip() for line in teste_sentences)\n","    # print(teste)\n","    # print(teste.find(\"synthesized\"))\n","    # print(teste.find(\"critic acid\"))\n","    # print(teste.find(\"ethanol\"))\n","    # print(teste.find(\"argon\"))\n","\n","    # from nltk.tokenize import word_tokenize, span_tokenize\n","    # nltk.download('punkt')\n","    # print(word_tokenize(\"Hello, my name is Arthur. How are you?\"))\n","    teste2 = list(twt().span_tokenize(teste))\n","    # print(teste2)\n","    # teste3 = list(twt().tokenize(teste))\n","    # print(teste3)\n","    # teste4 = list(twt().tokenize(\"Hello, my name is Arthur. How are you?\"))\n","    # print(teste4)\n","\n","    # print([teste[word_index[0]:word_index[1]] for word_index in teste2])\n","    print([(teste[word_index[0]:word_index[1]], word_index) for word_index in teste2])\n","\n","    line_counter = 0\n","\n","    # print(tags)\n","\n","    # print(teste2)\n","    for word_index in teste2:\n","        # print(line_counter, line_end_indexes, word_index[0])\n","        if word_index[0] > line_end_indexes[line_counter]:\n","            count = count+1\n","            line_counter = line_counter+1\n","\n","        word = teste[word_index[0]:word_index[1]]\n","        if('\"' in word):\n","            word = word.replace('\"', \"'\")\n","        if(\",\" in word or \"=\" in word):\n","            word = f'\"{word}\"'\n","\n","        # print(word)\n","        # print(word_index[0],word_index[1])\n","        \n","        found = False\n","        for tag in tags:\n","            if((word_index[0] >= tag[\"tags_indexes\"][0] and word_index[1] <= tag[\"tags_indexes\"][1]) or (word_index[0] >= tag[\"tags_indexes\"][0] and word_index[1]-1 <= tag[\"tags_indexes\"][1] and word[-1] == \".\")):\n","                # print(\"AEA!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","                # print(word_index)\n","                # print(word_index[0], word_index[1])\n","                # print(teste[word_index[0]:word_index[1]])\n","                # print(tag[\"tags_indexes\"])\n","                # print(tag[\"tag\"])\n","                # print(tag[\"word\"])\n","\n","                tag_prefix = \"B-\"\n","\n","                if result[-1][\"count\"] == count and result[-1][\"tag\"][2:] == tag[\"tag\"]:\n","                    tag_prefix = \"I-\"\n","\n","                if word[-1] == \".\":\n","                    # result.append(f'Sentence: {count},{word[:-1]},{tag[\"tag\"]}\\n')\n","                    result.append({\"count\": count, \"word\": word[:-1], \"tag\": f'{tag_prefix}{tag[\"tag\"]}'})\n","                    # result.append(f'Sentence: {count},{word[-1]},\"O\"\\n')\n","                    result.append({\"count\": count, \"word\": word[-1], \"tag\": \"O\"})\n","                else:\n","                    # result.append(f'Sentence: {count},{word},{tag[\"tag\"]}\\n')\n","                    result.append({\"count\": count, \"word\": word, \"tag\": f'{tag_prefix}{tag[\"tag\"]}'})\n","                found = True\n","                break\n","        if not found:\n","            # print(\"!!!!!!!!!!!!\")\n","            if word[-1] == \".\":\n","                # result.append(f'Sentence: {count},{word[:-1]},\"O\"\\n')\n","                result.append({\"count\": count, \"word\": word[:-1], \"tag\": \"O\"})\n","                # result.append(f'Sentence: {count},{word[-1]},\"O\"\\n')\n","                result.append({\"count\": count, \"word\": word[-1], \"tag\": \"O\"})\n","            else:\n","                # result.append(f'Sentence: {count},{word},\"O\"\\n')\n","                result.append({\"count\": count, \"word\": word, \"tag\": \"O\"})\n","        # print(result[-2:])\n","    f1.close()\n","    f2.close()\n","    count = count + 1\n","\n","result = [f'Sentence: {line[\"count\"]},{line[\"word\"]},{line[\"tag\"]}\\n' for line in result]\n","\n","# print(result)\n","new_file = open(\"validation_data.csv\", \"w\")\n","new_file.writelines(result)\n","new_file.close()\n","print(count)\n","print(testeeee)\n","print(testeeee2)\n","\n","# print(list(twt().span_tokenize(\"3. Percocet , 5/325 , 1-2 tabs PO q4-6h prn pain .\")))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jnma3DRbG83U","executionInfo":{"status":"ok","timestamp":1658184483122,"user_tz":240,"elapsed":395,"user":{"displayName":"Altigran Soares","userId":"17972430181076780193"}},"outputId":"c9df7e3d-4cac-4533-9a74-ca2790f9234b"},"id":"Jnma3DRbG83U","execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","Processing files: 101016jssc200704044.ann, 101016jssc200704044.txt\n","[{'tag': 'Operation', 'tags_indexes': (194, 202), 'word': ['prepared']}, {'tag': 'Meta', 'tags_indexes': (228, 255), 'word': ['solid-state', 'reaction', 'method']}, {'tag': 'Material', 'tags_indexes': (259, 262), 'word': ['air']}, {'tag': 'Operation', 'tags_indexes': (347, 352), 'word': ['mixed']}, {'tag': 'Operation', 'tags_indexes': (402, 410), 'word': ['prefired']}, {'tag': 'Operation', 'tags_indexes': (459, 465), 'word': ['ground']}, {'tag': 'Operation', 'tags_indexes': (467, 477), 'word': ['pelletized']}, {'tag': 'Operation', 'tags_indexes': (483, 491), 'word': ['calcined']}, {'tag': 'Operation', 'tags_indexes': (599, 607), 'word': ['obtained']}, {'tag': 'Operation', 'tags_indexes': (628, 634), 'word': ['ground']}, {'tag': 'Operation', 'tags_indexes': (639, 646), 'word': ['pressed']}, {'tag': 'Operation', 'tags_indexes': (726, 733), 'word': ['reduced']}, {'tag': 'Operation', 'tags_indexes': (842, 850), 'word': ['obtained']}, {'tag': 'Material', 'tags_indexes': (161, 173), 'word': ['SrMo1-xNixO4']}, {'tag': 'Material', 'tags_indexes': (303, 308), 'word': ['SrCO3']}, {'tag': 'Material', 'tags_indexes': (310, 314), 'word': ['MoO3']}, {'tag': 'Material', 'tags_indexes': (320, 322), 'word': ['Ni']}, {'tag': 'Material', 'tags_indexes': (446, 453), 'word': ['powders']}, {'tag': 'Material', 'tags_indexes': (580, 592), 'word': ['SrMo1-xNixO4']}, {'tag': 'Material-Descriptor', 'tags_indexes': (569, 578), 'word': ['compounds']}, {'tag': 'Material', 'tags_indexes': (613, 622), 'word': ['compounds']}, {'tag': 'Material', 'tags_indexes': (658, 665), 'word': ['pellets']}, {'tag': 'Material', 'tags_indexes': (713, 720), 'word': ['pellets']}, {'tag': 'Material', 'tags_indexes': (739, 744), 'word': ['H2/Ar']}, {'tag': 'Material', 'tags_indexes': (824, 836), 'word': ['SrMo1-xNixO3']}, {'tag': 'Property-Misc', 'tags_indexes': (134, 149), 'word': ['polycrystalline']}, {'tag': 'Material-Descriptor', 'tags_indexes': (291, 302), 'word': ['high-purity']}, {'tag': 'Material-Descriptor', 'tags_indexes': (323, 330), 'word': ['powders']}, {'tag': 'Amount-Misc', 'tags_indexes': (370, 391), 'word': ['desired', 'stoichiometry']}, {'tag': 'Number', 'tags_indexes': (414, 417), 'word': ['900']}, {'tag': 'Condition-Unit', 'tags_indexes': (421, 422), 'word': ['C']}, {'tag': 'Number', 'tags_indexes': (427, 429), 'word': ['24']}, {'tag': 'Condition-Unit', 'tags_indexes': (430, 431), 'word': ['h']}, {'tag': 'Number', 'tags_indexes': (495, 499), 'word': ['1000']}, {'tag': 'Number', 'tags_indexes': (501, 505), 'word': ['1100']}, {'tag': 'Number', 'tags_indexes': (510, 514), 'word': ['1200']}, {'tag': 'Condition-Unit', 'tags_indexes': (518, 519), 'word': ['C']}, {'tag': 'Number', 'tags_indexes': (524, 526), 'word': ['24']}, {'tag': 'Condition-Unit', 'tags_indexes': (527, 528), 'word': ['h']}, {'tag': 'Number', 'tags_indexes': (672, 674), 'word': ['10']}, {'tag': 'Number', 'tags_indexes': (691, 692), 'word': ['2']}, {'tag': 'Property-Unit', 'tags_indexes': (675, 677), 'word': ['mm']}, {'tag': 'Property-Unit', 'tags_indexes': (693, 695), 'word': ['mm']}, {'tag': 'Property-Type', 'tags_indexes': (678, 686), 'word': ['diameter']}, {'tag': 'Property-Type', 'tags_indexes': (696, 705), 'word': ['thickness']}, {'tag': 'Number', 'tags_indexes': (746, 747), 'word': ['5']}, {'tag': 'Amount-Unit', 'tags_indexes': (747, 748), 'word': ['%']}, {'tag': 'Number', 'tags_indexes': (750, 752), 'word': ['95']}, {'tag': 'Amount-Unit', 'tags_indexes': (752, 753), 'word': ['%']}, {'tag': 'Number', 'tags_indexes': (763, 766), 'word': ['920']}, {'tag': 'Condition-Unit', 'tags_indexes': (770, 771), 'word': ['C']}, {'tag': 'Number', 'tags_indexes': (776, 778), 'word': ['12']}, {'tag': 'Condition-Unit', 'tags_indexes': (779, 780), 'word': ['h']}, {'tag': 'Amount-Misc', 'tags_indexes': (264, 287), 'word': ['Appropriate', 'proportions']}]\n","[('10.1016/j.ssc.2007.04.044', (0, 25)), ('Influence', (26, 35)), ('of', (36, 38)), ('Ni', (39, 41)), ('doping', (42, 48)), ('on', (49, 51)), ('the', (52, 55)), ('properties', (56, 66)), ('of', (67, 69)), ('perovskite', (70, 80)), ('molybdates', (81, 91)), ('SrMo1-xNixO3', (92, 104)), ('(', (105, 106)), ('0.02', (106, 110)), ('<', (110, 111)), ('=x', (111, 113)), ('<', (113, 114)), ('=0.08', (114, 119)), (')', (119, 120)), ('A', (122, 123)), ('series', (124, 130)), ('of', (131, 133)), ('polycrystalline', (134, 149)), ('samples', (150, 157)), ('of', (158, 160)), ('SrMo1-xNixO4', (161, 173)), ('(', (173, 174)), ('0.02', (174, 178)), ('<', (178, 179)), ('=x', (179, 181)), ('<', (181, 182)), ('=0.08', (182, 187)), (')', (187, 188)), ('were', (189, 193)), ('prepared', (194, 202)), ('through', (203, 210)), ('the', (211, 214)), ('conventional', (215, 227)), ('solid-state', (228, 239)), ('reaction', (240, 248)), ('method', (249, 255)), ('in', (256, 258)), ('air.', (259, 263)), ('Appropriate', (264, 275)), ('proportions', (276, 287)), ('of', (288, 290)), ('high-purity', (291, 302)), ('SrCO3', (303, 308)), (',', (308, 309)), ('MoO3', (310, 314)), (',', (314, 315)), ('and', (316, 319)), ('Ni', (320, 322)), ('powders', (323, 330)), ('were', (331, 335)), ('thoroughly', (336, 346)), ('mixed', (347, 352)), ('according', (353, 362)), ('to', (363, 365)), ('the', (366, 369)), ('desired', (370, 377)), ('stoichiometry', (378, 391)), (',', (391, 392)), ('and', (393, 396)), ('then', (397, 401)), ('prefired', (402, 410)), ('at', (411, 413)), ('900', (414, 417)), ('[', (418, 419)), ('?', (419, 420)), (']', (421, 422)), ('C', (422, 423)), ('for', (424, 427)), ('24', (428, 430)), ('h.', (431, 433)), ('The', (434, 437)), ('obtained', (438, 446)), ('powders', (447, 454)), ('were', (455, 459)), ('ground', (460, 466)), (',', (466, 467)), ('pelletized', (468, 478)), (',', (478, 479)), ('and', (480, 483)), ('calcined', (484, 492)), ('at', (493, 495)), ('1000', (496, 500)), (',', (500, 501)), ('1100', (502, 506)), ('and', (507, 510)), ('1200', (511, 515)), ('[', (516, 517)), ('?', (517, 518)), (']', (519, 520)), ('C', (520, 521)), ('for', (522, 525)), ('24', (526, 528)), ('h', (529, 530)), ('with', (531, 535)), ('intermediate', (536, 548)), ('grinding', (549, 557)), ('twice.', (558, 564)), ('White', (565, 570)), ('compounds', (571, 580)), (',', (580, 581)), ('SrMo1-xNixO4', (582, 594)), (',', (594, 595)), ('were', (596, 600)), ('obtained.', (601, 610)), ('The', (611, 614)), ('compounds', (615, 624)), ('were', (625, 629)), ('ground', (630, 636)), ('and', (637, 640)), ('pressed', (641, 648)), ('into', (649, 653)), ('small', (654, 659)), ('pellets', (660, 667)), ('about', (668, 673)), ('10', (674, 676)), ('mm', (677, 679)), ('diameter', (680, 688)), ('and', (689, 692)), ('2', (693, 694)), ('mm', (695, 697)), ('thickness.', (698, 708)), ('These', (709, 714)), ('pellets', (715, 722)), ('were', (723, 727)), ('reduced', (728, 735)), ('in', (736, 738)), ('a', (739, 740)), ('H2/Ar', (741, 746)), ('(', (747, 748)), ('5', (748, 749)), ('%', (749, 750)), (':', (750, 751)), ('95', (752, 754)), ('%', (754, 755)), (')', (755, 756)), ('flow', (757, 761)), ('at', (762, 764)), ('920', (765, 768)), ('[', (769, 770)), ('?', (770, 771)), (']', (772, 773)), ('C', (773, 774)), ('for', (775, 778)), ('12', (779, 781)), ('h', (782, 783)), (',', (783, 784)), ('and', (785, 788)), ('then', (789, 793)), ('the', (794, 797)), ('deep', (798, 802)), ('red', (803, 806)), ('colored', (807, 814)), ('products', (815, 823)), ('of', (824, 826)), ('SrMo1-xNixO3', (827, 839)), ('were', (840, 844)), ('obtained', (845, 853)), ('.', (853, 854))]\n","2137\n","4\n","10\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"split.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":5}