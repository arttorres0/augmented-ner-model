{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFh7WVoJH5dr"
      },
      "source": [
        "Adapted from [ner_with_bilstm_and_crf](https://www.kaggle.com/nikkisharma536/ner-with-bilstm-and-crf/notebook)\n",
        "Altigran Soares da Silva\n",
        "IComp/UFAM - 15/03/2021\n"
      ],
      "id": "FFh7WVoJH5dr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "instant-coupon"
      },
      "outputs": [],
      "source": [
        "# Import libs\n",
        "# Also remember to use GPU in your colab notebook\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import math\n",
        "from math import nan\n",
        "import random\n",
        "import json\n",
        "from future.utils import iteritems\n",
        "import pickle\n",
        "\n",
        "# Needed for using CRF\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "import keras as k\n",
        "from keras_contrib.layers import CRF\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, classification_report"
      ],
      "id": "instant-coupon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mmt06ncv89hH"
      },
      "outputs": [],
      "source": [
        "# Code to read csv file from google drive into Colaboratory:\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id':\"1dE5_okk7cuLzqfNfd6ockHmj29o8PZ40\"})\n",
        "downloaded.GetContentFile('ner_medical.csv')\n",
        "\n",
        "# Read the csv file in a dataframe called \"data\"\n",
        "data = pd.read_csv(\"ner_medical.csv\", encoding=\"latin1\")\n",
        "# Fill NaN values using the specified method\n",
        "# Ffill propagate last valid observation/value forward to next valid \n",
        "data = data.fillna(method=\"ffill\")"
      ],
      "id": "Mmt06ncv89hH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adverse-doctor"
      },
      "outputs": [],
      "source": [
        "# Explore the input dataset\n",
        "print(\"Number of sentences: \", len(data.groupby(['Sentence #'])))\n",
        "\n",
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "print(\"Number of words in the dataset: \", n_words)\n",
        "\n",
        "tags = list(set(data[\"Tag\"].values))\n",
        "print(\"Tags:\", tags)\n",
        "n_tags = len(tags)\n",
        "print(\"Number of Labels: \", n_tags)\n",
        "\n",
        "print(\"What the dataset looks like:\")\n",
        "# Show the first 10 rows\n",
        "data.head(n=10)"
      ],
      "id": "adverse-doctor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "painful-karaoke"
      },
      "outputs": [],
      "source": [
        "# SentenceGetter re-organize \"data\" as an arry of setences\n",
        "# Each sentence is a list of pairs <word,tag> \n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                        s[\"Tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"Sentence #\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "getter = SentenceGetter(data)\n",
        "sentences = getter.sentences\n",
        "# Example: sentence #200 \n",
        "sentences[200]"
      ],
      "id": "painful-karaoke"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "round-providence"
      },
      "outputs": [],
      "source": [
        "# Explore set of sentences\n",
        "# Plot sentences by length\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(s) for s in sentences], bins=50)\n",
        "plt.title('Token per sentence')\n",
        "plt.xlabel('Len (number of token)')\n",
        "plt.ylabel('# samples')\n",
        "plt.show()"
      ],
      "id": "round-providence"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juvenile-scene"
      },
      "outputs": [],
      "source": [
        "# Keras (and most other ML packages) expect all the ids to be numeric, \n",
        "# this is an optimisation to save memory. \n",
        "# We will create the following dictionaries:\n",
        "# word2idx: assign a numeric index to each word in the dataset\n",
        "# idx2word: inverted version of word2idx\n",
        "# tag2idx: assign a numeric index to each tag in the dataset\n",
        "# idx2tag: inverted version of tag2idx:\n",
        "\n",
        "# words <= list of all words in the input dataset\n",
        "words = list(set(data[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "\n",
        "# tags <= list of all tags in the input dataset\n",
        "tags = []\n",
        "for tag in set(data[\"Tag\"].values):\n",
        "    if tag is nan or isinstance(tag, float):\n",
        "        tags.append('unk')\n",
        "    else:\n",
        "        tags.append(tag)\n",
        "n_tags = len(tags)\n",
        "\n",
        "# Dictionaries\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "idx2word = {i: w for w, i in iteritems(word2idx)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "idx2tag = {v: k for k, v in iteritems(tag2idx)}\n",
        "\n",
        "# Index number for the word 'delirium'\n",
        "print(word2idx['delirium'])\n",
        "# Word of index 10\n",
        "print(idx2word[10])\n",
        "# Index number for the tag 'B-Chemical'\n",
        "print(tag2idx['B-Chemical'])\n",
        "# Tag of index 4\n",
        "print(idx2tag[4])"
      ],
      "id": "juvenile-scene"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "delayed-dryer"
      },
      "outputs": [],
      "source": [
        "# Split train and test data\n",
        "X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
        "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "id": "delayed-dryer"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttsyh05Rhovo"
      },
      "outputs": [],
      "source": [
        "# Use this function to randomly remove some points from training dataset\n",
        "# Use removal percentage in decimal value. E.g.: if you set as 0.5, it will\n",
        "# remove 50% of the dataset\n",
        "\n",
        "def random_remove_data_points(dataset, labels, removal_percentage):\n",
        "    if removal_percentage < 0 or removal_percentage > 1:\n",
        "        raise Exception(\"Invalid removal percentage\")\n",
        "    \n",
        "    if removal_percentage == 1:\n",
        "        raise Exception(\"You can't remove the entire dataset\")\n",
        "    \n",
        "    number_of_points_remaining = math.ceil(len(dataset)*(1-removal_percentage))\n",
        "    random_idxs = np.random.choice(len(dataset), number_of_points_remaining, replace=False)\n",
        "    return [dataset[i] for i in random_idxs], [labels[i] for i in random_idxs]\n",
        "\n",
        "print(f\"Points in X_train before removal: {len(X_train)}\")\n",
        "print(f\"Points in y_train before removal: {len(y_train)}\")\n",
        "# X_train, y_train = random_remove_data_points(X_train, y_train, 0.5)\n",
        "print(f\"Points in X_train before removal: {len(X_train)}\")\n",
        "print(f\"Points in y_train before removal: {len(y_train)}\")"
      ],
      "id": "Ttsyh05Rhovo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SN-NYLpgsFa"
      },
      "outputs": [],
      "source": [
        "# Aux functions to save and load data and dicts, if data consistency is important\n",
        "# and there is desire to not random split again\n",
        "\n",
        "def save_backup_dataset(dataset, filename):\n",
        "  dataset_df = pd.DataFrame(dataset)\n",
        "  dataset_df.to_csv(filename, index=False)\n",
        "  gfile = drive.CreateFile({'parents': [{'id': '1iE8IHrWnp0dOZE9jQEgdX1zuoOVxucpi'}]})\n",
        "  gfile.SetContentFile(filename)\n",
        "  gfile.Upload()\n",
        "\n",
        "def save_backup_dict(dict, filename):\n",
        "  dict_file = open(filename, \"wb\")\n",
        "  pickle.dump(dict, dict_file)\n",
        "  dict_file.close()\n",
        "  gfile = drive.CreateFile({'parents': [{'id': '1iE8IHrWnp0dOZE9jQEgdX1zuoOVxucpi'}]})\n",
        "  gfile.SetContentFile(filename)\n",
        "  gfile.Upload()\n",
        "\n",
        "def get_backup_files_ids(folder_id):\n",
        "  file_list = drive.ListFile({'q': \"'{}' in parents and trashed=false\".format(folder_id)}).GetList()\n",
        "  return file_list\n",
        "\n",
        "def load_backup_dataset(file_id):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile(f\"{file_id}.csv\")\n",
        "\n",
        "  dataset = pd.read_csv(f\"{file_id}.csv\", encoding=\"latin1\")\n",
        "  dataset = dataset.fillna(method=\"ffill\")\n",
        "  dataset = dataset.values.tolist()\n",
        "  dataset = [ [ int(word) for word in sentence if str(word) != 'nan' ] for sentence in dataset]\n",
        "  return dataset\n",
        "\n",
        "def load_backup_dict(file_id):\n",
        "  downloaded = drive.CreateFile({'id':file_id})\n",
        "  downloaded.GetContentFile(f\"{file_id}.pkl\")\n",
        "\n",
        "  dict_file = open(f\"{file_id}.pkl\", \"rb\")\n",
        "  out_dict = pickle.load(dict_file)\n",
        "  return out_dict"
      ],
      "id": "0SN-NYLpgsFa"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uncomment this cell if you want to save data for further use\n",
        "\n",
        "# # Check some points before saving\n",
        "# print(X_train[0])\n",
        "# print(y_train[0])\n",
        "# print(X_test[0])\n",
        "# print(y_test[0])\n",
        "# print(word2idx['delirium'])\n",
        "# print(tag2idx['B-Chemical'])\n",
        "# print(idx2tag[2])\n",
        "# print(idx2word[100])\n",
        "\n",
        "# X_train_filename = 'X_train.csv'\n",
        "# y_train_filename = 'y_train.csv'\n",
        "# X_test_filename = 'X_test.csv'\n",
        "# y_test_filename = 'y_test.csv'\n",
        "\n",
        "# word2idx_filename = 'word2idx.pkl'\n",
        "# idx2word_filename = 'idx2word.pkl'\n",
        "# tag2idx_filename = 'tag2idx.pkl'\n",
        "# idx2tag_filename = 'idx2tag.pkl'\n",
        "\n",
        "# save_backup_dataset(X_train, X_train_filename)\n",
        "# save_backup_dataset(y_train, y_train_filename)\n",
        "# save_backup_dataset(X_test, X_test_filename)\n",
        "# save_backup_dataset(y_test, y_test_filename)\n",
        "\n",
        "# save_backup_dict(word2idx, word2idx_filename)\n",
        "# save_backup_dict(idx2word, idx2word_filename)\n",
        "# save_backup_dict(tag2idx, tag2idx_filename)\n",
        "# save_backup_dict(idx2tag, idx2tag_filename)"
      ],
      "metadata": {
        "id": "MzRQfI30tuI2"
      },
      "id": "MzRQfI30tuI2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvip_oC0j5-y"
      },
      "outputs": [],
      "source": [
        "# # Uncomment this cell if you want to load saved data\n",
        "\n",
        "# # Re-import necessary libs\n",
        "# import pandas as pd\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# import pickle, math\n",
        "# import numpy as np\n",
        "# %tensorflow_version 1.x\n",
        "\n",
        "# # Re-get important variables\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "# downloaded = drive.CreateFile({'id':\"1dE5_okk7cuLzqfNfd6ockHmj29o8PZ40\"})\n",
        "# downloaded.GetContentFile('ner_medical.csv')\n",
        "# data = pd.read_csv(\"ner_medical.csv\", encoding=\"latin1\")\n",
        "# data = data.fillna(method=\"ffill\")\n",
        "# words = list(set(data[\"Word\"].values))\n",
        "# n_words = len(words)\n",
        "# tags = list(set(data[\"Tag\"].values))\n",
        "# n_tags = len(tags)\n",
        "\n",
        "# backup_file_list = get_backup_files_ids('1iE8IHrWnp0dOZE9jQEgdX1zuoOVxucpi')\n",
        "\n",
        "# X_train_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == X_train_filename][0]['id']\n",
        "# y_train_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == y_train_filename][0]['id']\n",
        "# X_test_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == X_test_filename][0]['id']\n",
        "# y_test_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == y_test_filename][0]['id']\n",
        "\n",
        "# word2idx_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == word2idx_filename][0]['id']\n",
        "# idx2word_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == idx2word_filename][0]['id']\n",
        "# tag2idx_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == tag2idx_filename][0]['id']\n",
        "# idx2tag_file_id = [backup_file for backup_file in backup_file_list if backup_file['title'] == idx2tag_filename][0]['id']\n",
        "\n",
        "# X_train = load_backup_dataset(X_train_file_id)\n",
        "# y_train = load_backup_dataset(y_train_file_id)\n",
        "# X_test = load_backup_dataset(X_test_file_id)\n",
        "# y_test = load_backup_dataset(y_test_file_id)\n",
        "\n",
        "# word2idx = load_backup_dict(word2idx_file_id)\n",
        "# idx2word = load_backup_dict(idx2word_file_id)\n",
        "# tag2idx = load_backup_dict(tag2idx_file_id)\n",
        "# idx2tag = load_backup_dict(idx2tag_file_id)\n",
        "\n",
        "# # Check some points after loading data to see if they match the ones before saving\n",
        "# print(X_train[0])\n",
        "# print(y_train[0])\n",
        "# print(X_test[0])\n",
        "# print(y_test[0])\n",
        "\n",
        "# print(word2idx['delirium'])\n",
        "# print(tag2idx['B-Chemical'])\n",
        "# print(idx2tag[2])\n",
        "# print(idx2word[100])"
      ],
      "id": "zvip_oC0j5-y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux5-6tyMhovp"
      },
      "outputs": [],
      "source": [
        "# Aux function to help in augmentation. Generates a dict where entities\n",
        "# are the keys, and words are the values.\n",
        "\n",
        "def create_entities_dict(dataset, labels, decoded_word=False):\n",
        "    entities_dict = {}\n",
        "    \n",
        "    for i, sentence in enumerate(dataset):\n",
        "        for k, word in enumerate(sentence):\n",
        "            tag = idx2tag[labels[i][k]]\n",
        "            if tag[:2] == \"B-\":\n",
        "                if decoded_word:\n",
        "                    word_list = [idx2word[word]]\n",
        "                else:\n",
        "                    word_list = [word]\n",
        "                j = k + 1\n",
        "                if j < len(labels[i]):\n",
        "                    while idx2tag[labels[i][j]][:2] == \"I-\":\n",
        "                        if decoded_word:\n",
        "                            word_list.append(idx2word[dataset[i][j]])\n",
        "                        else:\n",
        "                            word_list.append(dataset[i][j])\n",
        "                        j = j+1\n",
        "                        if j == len(labels[i]):\n",
        "                            break\n",
        "                        \n",
        "                if entities_dict.get(tag):\n",
        "                    if word_list not in entities_dict[tag]:\n",
        "                        entities_dict[tag].append(word_list)\n",
        "                else:\n",
        "                    entities_dict[tag] = [word_list]\n",
        "                    \n",
        "    return entities_dict\n",
        "\n",
        "entities_dict = create_entities_dict(X_train, y_train)"
      ],
      "id": "ux5-6tyMhovp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wRVTj71hovp"
      },
      "outputs": [],
      "source": [
        "# Augmentation function using entity replacement technique.\n",
        "# It will generate a new dataset, with X% more points based on\n",
        "# the original dataset. E.g.: if you set augmentation percentage as 0.5 and dataset has\n",
        "# 1000 points, it will generate a dataset with 1500 points.\n",
        "\n",
        "def generate_sentences(X_train, y_train, entities_dict, augmented_set_size_percentage):\n",
        "    if augmented_set_size_percentage < 0:\n",
        "        raise Exception(\"Invalid augmented set size percentage\")\n",
        "\n",
        "    print(f\"Points in X_train before augmentation: {len(X_train)}\")\n",
        "    print(f\"Points in y_train before augmentation: {len(y_train)}\")\n",
        "\n",
        "    number_of_new_sentences = math.ceil(augmented_set_size_percentage * len(X_train))\n",
        "    random_idxs = np.random.choice(len(X_train), number_of_new_sentences, replace=True)\n",
        "    \n",
        "    base_sequences = [X_train[i] for i in random_idxs]\n",
        "    base_labels = [y_train[i] for i in random_idxs]\n",
        "\n",
        "    new_sequences = []\n",
        "    new_labels = []\n",
        "    \n",
        "    for k, sequence in enumerate(base_sequences):\n",
        "        new_sequence = []\n",
        "        new_label = []\n",
        "\n",
        "        for i, word in enumerate(sequence):\n",
        "            tag = idx2tag[base_labels[k][i]]\n",
        "            if tag == \"O\":\n",
        "                new_sequence.append(word)\n",
        "                new_label.append(base_labels[k][i])\n",
        "            elif tag[:2] == \"B-\":\n",
        "                same_entities_type = entities_dict[tag]\n",
        "                random_entity = np.random.choice(same_entities_type, 1)[0]\n",
        "                random_number_of_tokens = random.randint(1, len(random_entity))\n",
        "                random_entity_tokens = np.random.choice(random_entity, random_number_of_tokens, replace = False).tolist()\n",
        "                entity = tag[2:]\n",
        "                decoded_token_labels = [f\"I-{entity}\" for token in random_entity_tokens]\n",
        "                decoded_token_labels[0] = tag\n",
        "                encoded_token_labels = [tag2idx[label] for label in decoded_token_labels]\n",
        "                new_sequence = new_sequence + random_entity_tokens\n",
        "                new_label = new_label + encoded_token_labels\n",
        "\n",
        "        new_sequences.append(new_sequence)\n",
        "        new_labels.append(new_label)\n",
        "\n",
        "    augmented_X_train = X_train + new_sequences\n",
        "    augmented_y_train = y_train + new_labels\n",
        "\n",
        "    print(f\"Points in X_train after augmentation: {len(augmented_X_train)}\")\n",
        "    print(f\"Points in y_train after augmentation: {len(augmented_y_train)}\")\n",
        "\n",
        "    return augmented_X_train, augmented_y_train"
      ],
      "id": "2wRVTj71hovp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "limiting-africa"
      },
      "outputs": [],
      "source": [
        "# Graph plot function \n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    accuracy = history.history['accuracy']\n",
        "    val_accuracy = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(accuracy) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, accuracy, 'b', label='Training acc')\n",
        "    plt.plot(x, val_accuracy, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "limiting-africa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrdK597HE6vV"
      },
      "outputs": [],
      "source": [
        "# This function converts predicted values to labels from idx2tag\n",
        "# This is to improve the interpretability of the results\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i])\n",
        "        out.append(out_i)\n",
        "    return out"
      ],
      "id": "QrdK597HE6vV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Model creation function\n",
        "def create_model(maxlen, n_words):\n",
        "  input = Input(shape=(maxlen,))\n",
        "  word_embedding_size = 150\n",
        "\n",
        "  model = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=maxlen)(input)\n",
        "  model = Bidirectional(LSTM(units=word_embedding_size, \n",
        "                           return_sequences=True, \n",
        "                           dropout=0.5, \n",
        "                           recurrent_dropout=0.5, \n",
        "                           kernel_initializer=k.initializers.he_normal()))(model)\n",
        "  model = LSTM(units=word_embedding_size * 2, \n",
        "              return_sequences=True, \n",
        "              dropout=0.5, \n",
        "              recurrent_dropout=0.5, \n",
        "              kernel_initializer=k.initializers.he_normal())(model)\n",
        "  model = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  \n",
        "  crf = CRF(n_tags)\n",
        "  out = crf(model)\n",
        "  model = Model(input, out)\n",
        "\n",
        "  adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "  model.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "d8H1s-6b_-pM"
      },
      "id": "d8H1s-6b_-pM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training function\n",
        "def train(model, X_train_df, y_train_df):\n",
        "  filepath=\"ner-bi-lstm-td-model-{val_accuracy:.2f}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [checkpoint]\n",
        "\n",
        "  history = model.fit(X_train_df, np.array(y_train_df), batch_size=25, epochs=10, validation_split=0.2, verbose=1, callbacks=callbacks_list)\n",
        "  plot_history(history)"
      ],
      "metadata": {
        "id": "cjp-jXx4AmiV"
      },
      "id": "cjp-jXx4AmiV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model testing function\n",
        "def test(model, X_test_df, y_test_df):\n",
        "  test_pred = model.predict(X_test_df, verbose=1)   \n",
        "\n",
        "  pred_labels = pred2label(test_pred)\n",
        "  test_labels = pred2label(y_test_df)\n",
        "  \n",
        "  print(\"F1-score: {:.1%}\".format(f1_score(test_labels, pred_labels)))\n",
        "  report = classification_report(y_pred=pred_labels, y_true=test_labels, output_dict=True)\n",
        "  df = pd.DataFrame(report).transpose()\n",
        "  print(\"Classification Report:\")\n",
        "  print(df.to_csv())"
      ],
      "metadata": {
        "id": "JvdztU6FA8Bd"
      },
      "id": "JvdztU6FA8Bd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMknjbDrh6Fk"
      },
      "outputs": [],
      "source": [
        "def create_train_and_validate_model(augmented_percentage):\n",
        "  augmented_X_train, augmented_y_train = generate_sentences(X_train, y_train, entities_dict, augmented_percentage)\n",
        "\n",
        "  # Preparing data for training. \n",
        "  # X = observations vector. For text sequence labeling, observations are the index of the words in the sentences\n",
        "  # y = labels vector. For text sequence labeling, labels are the index of tags in the sentences\n",
        "  # The BI-LSTM layer expects all texts/sentences to be of the same length. \n",
        "  # So, we need to pad shorter sentences\n",
        "  # We select the padding size to be the length of the longest sentence.\n",
        "\n",
        "  maxlen_X_train = max([len(s) for s in augmented_X_train])\n",
        "  maxlen_X_test = max([len(s) for s in X_test])\n",
        "  maxlen_y_train = max([len(s) for s in augmented_y_train])\n",
        "  maxlen_y_test = max([len(s) for s in y_test])\n",
        "\n",
        "  maxlen = max([maxlen_X_train, maxlen_X_test, maxlen_y_train, maxlen_y_test])\n",
        "\n",
        "  augmented_X_train = pad_sequences(maxlen=maxlen, sequences=augmented_X_train, padding=\"post\",value=n_words - 1)\n",
        "  new_X_test = pad_sequences(maxlen=maxlen, sequences=X_test, padding=\"post\",value=n_words - 1)\n",
        "\n",
        "  augmented_y_train = pad_sequences(maxlen=maxlen, sequences=augmented_y_train, padding=\"post\", value=tag2idx[\"O\"])\n",
        "  new_y_test = pad_sequences(maxlen=maxlen, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"])\n",
        "\n",
        "  # Converts vector y (integers) to binary matrix.\n",
        "  # Each y is a one-hot vector where only the position of the tag gests \"1\"\n",
        "  augmented_y_train = [to_categorical(i, num_classes=n_tags) for i in augmented_y_train]\n",
        "  new_y_test = [to_categorical(i, num_classes=n_tags) for i in new_y_test]\n",
        "\n",
        "  model = None\n",
        "  model = create_model(maxlen, n_words)\n",
        "\n",
        "  train(model, augmented_X_train, augmented_y_train)\n",
        "\n",
        "  test(model, new_X_test, new_y_test)"
      ],
      "id": "jMknjbDrh6Fk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bM0wPLD5kaw4"
      },
      "outputs": [],
      "source": [
        "number_of_training_models = 10\n",
        "target_augmented_percentage = 0.5\n",
        "\n",
        "print(f\"!!!!!! Augmented Percentage {target_augmented_percentage*100}% !!!!!!\")\n",
        "\n",
        "for i in range(number_of_training_models):\n",
        "  print(f\"!!!!!! Starting model number {i+1} !!!!!!\")\n",
        "  create_train_and_validate_model(target_augmented_percentage)"
      ],
      "id": "bM0wPLD5kaw4"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "github.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}